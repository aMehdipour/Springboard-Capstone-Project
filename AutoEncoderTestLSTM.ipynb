{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder Developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d48baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import keras_tuner as kt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV,train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,r2_score,roc_auc_score,precision_score,recall_score,f1_score,mean_squared_error\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48dd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 'T2', 'T24'\n",
    "             , 'T30', 'T50','P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30'\n",
    "             , 'phi', 'NRf', 'NRc', 'BPR','farB', 'htBleed', 'Nf_dmd',\n",
    "             'PCNfR_dmd','W31', 'W32', 's22', 's23']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90d235",
   "metadata": {},
   "source": [
    "Import the training, test, and remaining useful life data and remove all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cb9faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remaining_cycles</th>\n",
       "      <th>Nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remaining_cycles  Nan\n",
       "0               112  NaN\n",
       "1                98  NaN\n",
       "2                69  NaN\n",
       "3                82  NaN\n",
       "4                91  NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.txt\", sep=' ', names=col_names)\n",
    "test = pd.read_csv(\"test.txt\", sep=' ', names=col_names)\n",
    "RUL = pd.read_csv(\"RUL.txt\", sep=' ', names=['remaining_cycles', 'Nan'])\n",
    "RUL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9224d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['s22', 's23'], axis=1, inplace=True)\n",
    "test.drop(columns=['s22', 's23'], axis=1, inplace=True)\n",
    "RUL.drop(columns=['Nan'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6ccb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>remaining_cycles</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   remaining_cycles  id\n",
       "0               112   1\n",
       "1                98   2\n",
       "2                69   3\n",
       "3                82   4\n",
       "4                91   5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUL['id'] = RUL.index + 1\n",
    "\n",
    "RUL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a7399",
   "metadata": {},
   "source": [
    "The dataset is made up of sensor data for 100 different engines, as can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc5c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100]\n"
     ]
    }
   ],
   "source": [
    "print('Engine IDs:', train.id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad2e86",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb2c6e",
   "metadata": {},
   "source": [
    "A testing dataset is provided where the engines are not ran until failure, and a complimentary vector is provided for the remaining useful life (RUL) for each engine ID. In order to match the engines in the test set with their corresponding RUL, we must concatenate the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34cf0cb",
   "metadata": {},
   "source": [
    "Since the engines in the test set are ran until failure, you can calculate the remaining cycles until failure by subtracting the current cycle from the maximum cycle for each engine ID. He does this for the test set as well. Since the remaining useful life for all the test engines are given, as well as their current cycle, the maximum cycles for each engine can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53a4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['remaining_cycles'] = train.groupby(['id'])['cycle'].transform(max)-train['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b33f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum cycles = cycles at test end + remaining useful life\n",
    "\n",
    "maxCycles = pd.DataFrame(test.groupby('id')['cycle'].max()).reset_index()\n",
    "maxCycles.columns = ['id', 'max_tested']\n",
    "maxCycles['max_cycles'] = RUL['remaining_cycles'] + maxCycles['max_tested']\n",
    "maxCycles.drop(['max_tested'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f53584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>max_cycles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  max_cycles\n",
       "0   1         143\n",
       "1   2         147\n",
       "2   3         195\n",
       "3   4         188\n",
       "4   5         189"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxCycles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a58ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(maxCycles, on=['id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f89b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['remaining_cycles'] = test['max_cycles'] - test['cycle']\n",
    "test.drop(['max_cycles'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train.drop(['remaining_cycles'], axis=1)\n",
    "y = train.remaining_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arash\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "meanTrain = Xtrain.loc[:, Xtrain.columns != 'cycle'].rolling(10).mean()\n",
    "meanTrain = meanTrain.join(Xtrain['cycle'])\n",
    "X = meanTrain[Xtrain.columns]\n",
    "X.dropna(inplace=True)\n",
    "y = y[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arash\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\preprocessing\\_data.py:3251: RuntimeWarning: divide by zero encountered in log\n",
      "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "gen = MinMaxScaler(feature_range=(0,1))\n",
    "X = gen.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "X = np.nan_to_num(X)\n",
    "\n",
    "pt = PowerTransformer()\n",
    "X = pt.fit_transform(X)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR = 10\n",
    "\n",
    "def create_dataset(X, AR=AR):\n",
    "    data = []\n",
    "    for i in range(len(X)-AR-1):\n",
    "        data.append(X[i:(i+AR)])\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_dataset(X, AR)\n",
    "y_train = y[AR+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20611, 10, 26)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderLSTM(Model):\n",
    "    def __init__(self, latent_dim, input_dim):\n",
    "        super(AutoEncoderLSTM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder.add(layers.LSTM(input_dim[1], activation=\"relu\", input_shape=(input_dim[1], input_dim[2]), return_sequences=True))\n",
    "        self.encoder.add(layers.LSTM(int(input_dim[1]/2), activation=\"relu\", return_sequences=True))\n",
    "        self.encoder.add(layers.LSTM(latent_dim, activation=\"relu\"))\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential()\n",
    "        self.decoder.add(layers.RepeatVector(input_dim[1]))\n",
    "        self.decoder.add(layers.LSTM(input_dim[1], activation=\"relu\", input_shape=(input_dim[1], input_dim[2]), return_sequences=True))\n",
    "        self.decoder.add(layers.TimeDistributed(layers.Dense(input_dim[2])))\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_model(autoencoder, loss_type, activation_type, input):\n",
    "    autoencoder.encoder.trainable = False\n",
    "    model = keras.Sequential()\n",
    "    model.add(autoencoder.encoder)\n",
    "    model.add(layers.RepeatVector(input[2]))\n",
    "    model.add(layers.LSTM(128, activation=activation_type, return_sequences=True))\n",
    "    model.add(layers.LSTM(64, activation=activation_type, return_sequences=False))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(16)))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(1)))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(fit):\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.plot(fit.history['loss'], label='loss')\n",
    "    plt.plot(fit.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Remaining Cycles Error')\n",
    "    plt.legend(loc=0, prop={'size':26})\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20611, 10, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder_lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 9)                 2340      \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (20611, 10, 26)           1086      \n",
      "=================================================================\n",
      "Total params: 3,426\n",
      "Trainable params: 3,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 9\n",
    "tf.keras.backend.clear_session()\n",
    "autoencoder = AutoEncoderLSTM(latent_dim,X.shape)\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "autoencoder.build(X.shape)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "645/645 [==============================] - 9s 9ms/step - loss: 0.4218 - val_loss: 0.3002\n",
      "Epoch 2/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.2847 - val_loss: 0.2710\n",
      "Epoch 3/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.2598 - val_loss: 0.2486\n",
      "Epoch 4/1000\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.2387 - val_loss: 0.2318\n",
      "Epoch 5/1000\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.2252 - val_loss: 0.2195\n",
      "Epoch 6/1000\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 0.2125 - val_loss: 0.2090\n",
      "Epoch 7/1000\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.2064 - val_loss: 0.2060\n",
      "Epoch 8/1000\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 0.2041 - val_loss: 0.2034\n",
      "Epoch 9/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.2028 - val_loss: 0.2037\n",
      "Epoch 10/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.2018 - val_loss: 0.2054\n",
      "Epoch 11/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.2009 - val_loss: 0.2014\n",
      "Epoch 12/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.2003 - val_loss: 0.2005\n",
      "Epoch 13/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1996 - val_loss: 0.1990\n",
      "Epoch 14/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1989 - val_loss: 0.2056\n",
      "Epoch 15/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1965 - val_loss: 0.1947\n",
      "Epoch 16/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1903 - val_loss: 0.1876\n",
      "Epoch 17/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1870 - val_loss: 0.1865\n",
      "Epoch 18/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1849 - val_loss: 0.1836\n",
      "Epoch 19/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1837 - val_loss: 0.1838\n",
      "Epoch 20/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1825 - val_loss: 0.1832\n",
      "Epoch 21/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.1815 - val_loss: 0.1830\n",
      "Epoch 22/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1800 - val_loss: 0.1794\n",
      "Epoch 23/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1787 - val_loss: 0.1790\n",
      "Epoch 24/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1772 - val_loss: 0.1815\n",
      "Epoch 25/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1757 - val_loss: 0.1759\n",
      "Epoch 26/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1748 - val_loss: 0.1750\n",
      "Epoch 27/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1738 - val_loss: 0.1765\n",
      "Epoch 28/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1731 - val_loss: 0.1738\n",
      "Epoch 29/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1723 - val_loss: 0.1722\n",
      "Epoch 30/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1717 - val_loss: 0.1710\n",
      "Epoch 31/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1711 - val_loss: 0.1705\n",
      "Epoch 32/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1700 - val_loss: 0.1695\n",
      "Epoch 33/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1692 - val_loss: 0.1705\n",
      "Epoch 34/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1685 - val_loss: 0.1699\n",
      "Epoch 35/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1677 - val_loss: 0.1686\n",
      "Epoch 36/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1671 - val_loss: 0.1664\n",
      "Epoch 37/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1665 - val_loss: 0.1667\n",
      "Epoch 38/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1660 - val_loss: 0.1664\n",
      "Epoch 39/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1656 - val_loss: 0.1663\n",
      "Epoch 40/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1651 - val_loss: 0.1672\n",
      "Epoch 41/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1647 - val_loss: 0.1648\n",
      "Epoch 42/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1644 - val_loss: 0.1648\n",
      "Epoch 43/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.1641 - val_loss: 0.1668\n",
      "Epoch 44/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.1637 - val_loss: 0.1638\n",
      "Epoch 45/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1635 - val_loss: 0.1653\n",
      "Epoch 46/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1632 - val_loss: 0.1638\n",
      "Epoch 47/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1629 - val_loss: 0.1632\n",
      "Epoch 48/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1627 - val_loss: 0.1735\n",
      "Epoch 49/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1627 - val_loss: 0.1652\n",
      "Epoch 50/1000\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 0.1622 - val_loss: 0.1621\n",
      "Epoch 51/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1622 - val_loss: 0.1639\n",
      "Epoch 52/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1620 - val_loss: 0.1618\n",
      "Epoch 53/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1616 - val_loss: 0.1628\n",
      "Epoch 54/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1616 - val_loss: 0.1630\n",
      "Epoch 55/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1613 - val_loss: 0.1620\n",
      "Epoch 56/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1610 - val_loss: 0.1617\n",
      "Epoch 57/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1611 - val_loss: 0.1630\n",
      "Epoch 58/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1608 - val_loss: 0.1618\n",
      "Epoch 59/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1607 - val_loss: 0.1614\n",
      "Epoch 60/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1606 - val_loss: 0.1623\n",
      "Epoch 61/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1604 - val_loss: 0.1663\n",
      "Epoch 62/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1603 - val_loss: 0.1613\n",
      "Epoch 63/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1601 - val_loss: 0.1606\n",
      "Epoch 64/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1601 - val_loss: 0.1684\n",
      "Epoch 65/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1600 - val_loss: 0.1613\n",
      "Epoch 66/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1598 - val_loss: 0.1647\n",
      "Epoch 67/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1598 - val_loss: 0.1654\n",
      "Epoch 68/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1596 - val_loss: 0.1603\n",
      "Epoch 69/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1596 - val_loss: 0.1592\n",
      "Epoch 70/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1594 - val_loss: 0.1627\n",
      "Epoch 71/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1594 - val_loss: 0.1591\n",
      "Epoch 72/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1591 - val_loss: 0.1596\n",
      "Epoch 73/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1590 - val_loss: 0.1594\n",
      "Epoch 74/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1589 - val_loss: 0.1623\n",
      "Epoch 75/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1590 - val_loss: 0.1599\n",
      "Epoch 76/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1588 - val_loss: 0.1671\n",
      "Epoch 77/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1589 - val_loss: 0.1761\n",
      "Epoch 78/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1589 - val_loss: 0.1579\n",
      "Epoch 79/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1587 - val_loss: 0.1595\n",
      "Epoch 80/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1586 - val_loss: 0.1600\n",
      "Epoch 81/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1584 - val_loss: 0.1623\n",
      "Epoch 82/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1585 - val_loss: 0.1587\n",
      "Epoch 83/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1583 - val_loss: 0.1592\n",
      "Epoch 84/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1581 - val_loss: 0.1602\n",
      "Epoch 85/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1583 - val_loss: 0.1578\n",
      "Epoch 86/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1581 - val_loss: 0.1582\n",
      "Epoch 87/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1579 - val_loss: 0.1594\n",
      "Epoch 88/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1580 - val_loss: 0.1609\n",
      "Epoch 89/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1581 - val_loss: 0.1596\n",
      "Epoch 90/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1577 - val_loss: 0.1578\n",
      "Epoch 91/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1576 - val_loss: 0.1584\n",
      "Epoch 92/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1577 - val_loss: 0.1579\n",
      "Epoch 93/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1576 - val_loss: 0.1591\n",
      "Epoch 94/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1576 - val_loss: 0.1584\n",
      "Epoch 95/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1575 - val_loss: 0.1576\n",
      "Epoch 96/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1574 - val_loss: 0.1581\n",
      "Epoch 97/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1573 - val_loss: 0.1582\n",
      "Epoch 98/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1572 - val_loss: 0.1572\n",
      "Epoch 99/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1571 - val_loss: 0.1575\n",
      "Epoch 100/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1570 - val_loss: 0.1586\n",
      "Epoch 101/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1571 - val_loss: 0.1582\n",
      "Epoch 102/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1572 - val_loss: 0.1578\n",
      "Epoch 103/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1568 - val_loss: 0.1564\n",
      "Epoch 104/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1569 - val_loss: 0.1580\n",
      "Epoch 105/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1568 - val_loss: 0.1581\n",
      "Epoch 106/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1569 - val_loss: 0.1614\n",
      "Epoch 107/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1569 - val_loss: 0.1577\n",
      "Epoch 108/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1568 - val_loss: 0.1567\n",
      "Epoch 109/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1567 - val_loss: 0.1579\n",
      "Epoch 110/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1566 - val_loss: 0.1568\n",
      "Epoch 111/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1565 - val_loss: 0.1558\n",
      "Epoch 112/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1566 - val_loss: 0.1604\n",
      "Epoch 113/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1565 - val_loss: 0.1568\n",
      "Epoch 114/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1566 - val_loss: 0.1564\n",
      "Epoch 115/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1564 - val_loss: 0.1581\n",
      "Epoch 116/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1564 - val_loss: 0.1611\n",
      "Epoch 117/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1563 - val_loss: 0.1563\n",
      "Epoch 118/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1562 - val_loss: 0.1589\n",
      "Epoch 119/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1563 - val_loss: 0.1562\n",
      "Epoch 120/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1561 - val_loss: 0.1582\n",
      "Epoch 121/1000\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 0.1561 - val_loss: 0.1570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10)\n",
    "autoencoder.fit(X, X, epochs=1000, shuffle=True, validation_data=(X, X), verbose=True, callbacks=callback)\n",
    "autoencoder.encoder.trainable = False\n",
    "autoencoder.encoder.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 10)            1480      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 5)             320       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 9)                 540       \n",
      "=================================================================\n",
      "Total params: 2,340\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,340\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39708/2176284142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstmModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_lstm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mean_squared_error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlstmModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlstmModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39708/292688553.py\u001b[0m in \u001b[0;36mmake_lstm_model\u001b[1;34m(autoencoder, loss_type, activation_type, input)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lstmModel = make_lstm_model(autoencoder, \"mean_squared_error\", \"sigmoid\", [X.shape[2], X.shape[1]])\n",
    "lstmModel.build([X.shape[2], X.shape[1]])\n",
    "lstmModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n",
    "history = lstmModel.fit(X, y, batch_size=512, epochs=500, validation_split=0.15, callbacks=callback, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cf77d9e31fba3236aefb4748d140888e596bc65dcef8da4aa710fb6056a88b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
